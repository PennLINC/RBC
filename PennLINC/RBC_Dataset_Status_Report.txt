RBC Dataset Status Report (DSR)

CCNP (GONG)
RIA Store: ssh://sciget.pmacs.upenn.edu/project/RBC/RIA/BIDS/970/4eab1-3320-41f0-9fce-3c6961b4ac96
Cubic Path: /cbica/projects/RBC/RBC_RAWDATA/bidsdatasets
Final CuBIDS Outputs: https://github.com/PennLINC/RBC/tree/master/PennLINC/CCNP_BIDS_Fix
Datalad Status: saved, clean
Git Branch: master
DSR: Audit ran successfully on both fieldmapped and fieldmappless fmriprep production outputs. Ran XCP on fieldmapped outputs and audited.
Next Steps: Compare old vs new XCP (Max). Check if brain_plot/reho step has a memory issue (Azeez). Compare fieldmappless vs fieldmapped in csdsi dataset. Rerun the 9 fieldmapped subjects who failed on XCP. Add README + Authors.

HRC (GONG)
RIA Store: ssh://sciget.pmacs.upenn.edu/project/RBC/RIA/BIDS/98f/55d34-d001-40ac-8056-db2803d3280f
Cubic Path: /cbica/projects/RBC/RBC_RAWDATA/bidsdatasets/HRC
Datalad Status: unlocked
Git Branch: main
DSR: We ran the entire dataset through multisession fmriprep. Testing/tweaking multisession fmriprep audit now.
Next Steps: If we decide to use fieldmapless (post running XCP on CCNP fieldmapless fmriprep outputs), audit the fieldmapless/multi-session fmriprep outputs. Run the entire dataset through fieldmapless/multi-session fmriprep. Audit. Then move onto XCP. Audit. 
Data Narrative: https://github.com/PennLINC/HRC/blob/main/working/curation_code/DataNarrative.md

PNC (GONG)
TODO: DECIDE IF WE WANT TO USE NEW OR OLD XCP (AZEEZ AND MAX, CHECK CORRELATION BETWEEN OLD AND NEW ON CCNP BEFORE USING NEW ON PNC)
RIA Store: ssh://sciget.pmacs.upenn.edu/project/RBC/RIA/BIDS/6a5/7d847-1441-442d-81d7-04ca51526645
Cubic Path: /cbica/projects/RBC/RBC_RAWDATA/bidsdatasets/PNC
Datalad Status: saved, clean
Git Branch: master
DSR: Fmriprep finished running on the entire dataset. Ran the new bootstrap version of the audit + concatenator on the fmriprep outputs, which revealed that all subject succeeded except the 40 with no bold data and one with a garbage T1w scan. Max ran XCP on the fmriprep outputs. Testing the XCP bootstrap audit now.  
Next Steps: Hold off on XCP audit until we answer 3 XCP questions on CCNP. Once it passes a single subject test, run the bootstrap XCP audit on the entire output set. Run qsiprep on exemplars, once Matt is done with run by session and patch 2 self. 
Data Narrative: /cbica/projects/RBC/PNC/working/curation_code/DataNarrative.md

HBN
RIA Store: ssh://sciget.pmacs.upenn.edu/project/RBC/RIA/BIDS/b32/508c1-d2c1-4b40-a3e3-48df49e7fa56
Cubic Path: /cbica/projects/RBC/RBC_RAWDATA/bidsdatasets/HBN
Datalad Status: untracked changes: removed 32 new subjects, purged 9 acq-dwi unused fmaps, removed 4 site-RU subjects with only 1/2 a functional scan.
Git Branch: main
DSR:  Per Alex's request, deleted the following 4 site-RU subjects, since they only have 1/2 a functional scan: sub-NDARCT301RK9, sub-NDARKT032RYR, sub-NDARNF449FMU, sub-NDARYU575CE2. Uploaded the 32 cbic subjects from aws we were missing that were scanned with abcd protocol. Validate revealed the new 32 were not at all bids valid. Spoke with Alex, and he asked us to validate them. Added a session directory and added the session name to the filepaths. Group/validate ran. Qsubbed purge on all PD scans in the 32 new cbic/abcd subjcts. REMOVED 32 new cbic subjects (decided to add back in round 2 of HBN data). Purged 9 acq-dwi fmap scans no longer being used (their subjects have no dwi data), at Alex's request. Group/validate running now. Save ran on fix-anats. Merged fix-anats into main (cus Matt deleted the extra T1w for sub-NDARXJ973GFA, which had two T1ws that looked like different people, on that branch, and I made the following changes on that branch: removed 32 new subjects, purged 9 acq-dwi unused fmaps, removed 4 site-RU subjects with only 1/2 a functional scan. Tinashe added subject level events tsvs, and I fixed them to include the end time of the movie and a trial_type column. Also added top level task-peer_events.tsv file and renamed the third column to "trial_type." Group came out clean. Validate running now. 
Git Branch: main. 
Next Steps: Save. Rename 23 abcd-cbic subjects to have acq-ABCD. Use tolerances listed below, apply changes, and push back to PMACS. 
Data Narrative: https://github.com/PennLINC/RBC/blob/master/PennLINC/HBN_BIDS_Fix/HBN_Data_Narrative.md

NKI
Datalad Status: saved, clean
Git Branch: apply1
Cubic Path: /cbica/projects/RBC/RBC_RAWDATA/bidsdatasets/NKI
DSR: Alex regenerated the correct jsons for the 117/125 T2w scans that were incorrect and gave us the go ahead to copy existing T2w sidecars to create the remaining 8 jsons that needed to be replaced, since the T2ws all used the same protocol. We replaced the incorrect sidecars with the new ones and ran add-nifti-info. Group/validate ran overnight. Copied sub-A00031794_ses-FLU1_T2w.json to replace the 8 T2w scans with missing sidecars still and ran add nifti info. Group/validate running came out clean! Save finished. Purged 25 task-pCASL (pseudo-continuous ASL) scans. Group/validate came out clean. Saved. We then increase the duration of the task-rest subject level events tsvs to match the lengths of their corresponding scans. Group/validate running now (with config params). Removed 84 subject level task-rest tsvs. Group/validate running now with tolerances. Save ran. Checked out apply1 branch and qsubbed apply! Ran for 3 days and didn't finish. Killed apply and re-qsubbed with 64G memory (instead of 32).
Next Steps: Rerun group/validate (group with NKI config). Save. Get Alex and Ted to sign off on post-apply output! Create sibling and push to pmacs! Merge apply1 into main. Push to pmacs again!


NKI TOLERANCES (from meeting with Alex)
DWI: tolerate 2.4-2.8 TR, tolerate all echo time

HBN FINAL TOLERNACES 
ANAT: EchoTime: 0.002, 
DWI: group 1 = CBIC/CUNY, group 2 = RU, group 3 = SI, EES, TRT, TR 
FMAP: VoxelSizeDim*:0.05mm, EES:0.0001, TRT: 0.01 
FUNC: EES:precision=4, TRT:precision=2

HBN ISSUES (CURRENT) 
- 29/32 of the new cbic subjects scanned with abcd protocl have the slice time greater than TR issue. There are 4 subjects with no bold niftis (only sidecars), and 3 out of these 4 are the only 3 subjects that do not have the large slice timing issue. Therefore, each of the 32 new subjects has EITHER the slice timing issue OR no bold niftis. 



HBN ISSUES (OLD)
- ANAT: Change T1 key group with no acq value to acquisition-SI
- ANAT: Need to delete some T2s, waiting on CMI for which ones (Mike or Alex)
- ANAT: RULE FOR T1s: Ideally, keep the VnavNorm scan with the highest rating. If unscored, keep best one (from plot), if can't tell difference, keep first run. If no VnavNorm, keep Vnav. If neither, keep HCP. 
- DWI + FMAP: tolerate Effective Echo Spacing, Total Readout Time, and Repetition Time
- FMAP: lots of missing PRFIP and MAF, want this in filename? want separate group because of this? SHORTHAND FOR ACQUISITION (UNSET SUGGEST VARIANT RENAME)! 
- FMAP: tolerate VoxelSizeDim1 (0.05mm)
- ADD PixelBandwith to sidecar params!!!!
- RERUN GROUP WITH SES INCLUDED IN GROUPINGS! 

NKI BOLD (DELETED)
(1) task-checkerboard scans with less than the dominant group's quantity of data
- 19 acq-645 check scans with less than the dominant group's quantity of data: 240*0.645/60 = 2.58 mins of data
- 8 check acq-1400 check scans with less than the dominant group's quantity of data: 98*1.4/60 = 2.28666666667 mins of data
- 2 acq-1400RR scans with less than the dominant group's quantity of data: 98*1.4/60 = 2.28666666667 mins of data
- 2 check acq-65RR scans with less than the dominant group's quantity of data: 240*0.645/60 = 2.58 mins of data

(2) 60 task-rest scans with less than 3 mins of data 

(3) task-breathhold 
- 3 breath acq-1400RR scans with less than the dominant group's quantity of data: 191*1.4/60 = 4.45666666667 mins of data
- 20 acq-1400 breathhold scans that have less than the dominant group's quantity of data: 186*1.4/60 = 4.34 mins of data
The task-breathhold dominant groups have more than 3 mins of data. 


HRC ISSUES
- VALIDATION: 11 new subs still quick validation failing ('bids dir structure doesn't match' description so maybe just having an issue with ses-3?) using the --sequential flag! Matt and I found nothing wrong with them (likely just need to debug the new flag. They passed validation WITHOUT the sequential flag. 
