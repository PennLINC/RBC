{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading An ASL Acquisition\n",
    "\n",
    "Typically on Flywheel, we can upload BIDS ready data using `fw bids import`. Unfortunately, in this case, ASL data doesn't yet have official BIDS specifications -- this means Flywheel doesn't know how to read it in automatically!\n",
    "\n",
    "```\n",
    "(base) ttapera@dopamine:/storage/ttapera/RBC/data$ fw import bids --debug --project RBC_PNC /storage/ttapera/RBC/data/131160/ bbl\n",
    "DEBUG: CLI Version: 12.1.1\n",
    "DEBUG: CLI Args: ['/home/ttapera/.cache/flywheel/python-3.6.6/lib/python3.6/site-packages/flywheel_cli/main.pyc', 'import', 'bids', '--debug', '--project', 'RBC_PNC', '131160/', 'bbl']\n",
    "DEBUG: Platform: Linux-4.4.0-177-generic-x86_64-with-debian-stretch-sid\n",
    "DEBUG: System Encoding: UTF-8\n",
    "DEBUG: Python Version: 3.6.6 (default, Jun 27 2018, 22:42:57)\n",
    "[GCC 6.4.0]\n",
    "DEBUG: SDK Version: 12.1.0\n",
    "DEBUG: Flywheel Site URL: https://upenn.flywheel.io:443/api\n",
    "INFO: Verifying directory exists\n",
    "INFO: Project (RBC_PNC) was found. Adding data to existing project.\n",
    "WARNING: Project has enabled rules, these may overwrite BIDS data. Either disable rules or run bids curation gear after data is uploaded.\n",
    "Continue upload? (yes/no): yes\n",
    "INFO: Subject (sub-2791617373) was found. Adding data to existing subject.\n",
    "INFO: Session (ses-PNC1) for subject (sub-2791617373) was found. Adding data to existing session.\n",
    "INFO: Acquisition (m0scan) not found. Creating new acquisition for session 5f130e858a33e0393ed3495a.\n",
    "INFO: Acquisition (acq-se_asl) not found. Creating new acquisition for session 5f130e858a33e0393ed3495a.\n",
    "INFO: Acquisition (acq-gre_asl) not found. Creating new acquisition for session 5f130e858a33e0393ed3495a.\n",
    "DEBUG: Uncaught Exception\n",
    "Traceback (most recent call last):\n",
    "  File \"flywheel_cli/main.pyc\", line 62, in main\n",
    "  File \"flywheel_cli/commands/import_bids.pyc\", line 60, in import_bids\n",
    "  File \"flywheel_bids/upload_bids.pyc\", line 1037, in upload_bids\n",
    "  File \"flywheel_bids/upload_bids.pyc\", line 698, in upload_bids_dir\n",
    "  File \"flywheel_bids/upload_bids.pyc\", line 567, in handle_subject_folder\n",
    "  File \"flywheel_bids/upload_bids.pyc\", line 306, in upload_acquisition_file\n",
    "  File \"flywheel_bids/upload_bids.pyc\", line 357, in classify_acquisition\n",
    "AttributeError: 'NoneType' object has no attribute 'get'\n",
    "Error: 'NoneType' object has no attribute 'get'\n",
    "Flywheel CLI 12.1.1 build a8ff8ea35efce4642e35a9221891b30c31857b60 on 2020-06-19 19:41\n",
    "```\n",
    "\n",
    "In this notebook we demonstrate how to create a custom parser & uploader for ASL data.\n",
    "\n",
    "The general strategy is as follows:\n",
    "\n",
    "1. List all the files we want to import\n",
    "2. Get the target _session_ object to which we want to upload these files\n",
    "3. Organise the files into _acquisitions_ that we can create objects out of\n",
    "4. Create _acquisition_ objects in the session\n",
    "5. Upload the appropriate files to the correct acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up the flywheel client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flywheel\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "\n",
    "client = flywheel.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some arguments that go into this script; no need to worry about these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = '131160,2791617373,sub-2791617373,True'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['/storage/ttapera/RBC/PennLINC/Transfer/renameDWI.py'] + args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/storage/ttapera/RBC/PennLINC/Transfer/renameDWI.py',\n",
       " '131160',\n",
       " '2791617373',\n",
       " 'sub-2791617373',\n",
       " 'True']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the files we need to import in BIDS; we use `glob` to do an easy listing of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/storage/ttapera/RBC/data/{}/*/*/*/*'.format(args[1])\n",
    "\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.json',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.json',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.nii.gz',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_aslcontext.tsv',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.json',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.nii.gz',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_aslcontext.tsv',\n",
       " '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.nii.gz']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an `acquisition` object; we look at this folder of files and create a function for extracting the subject and session labels from the filepath using `regex`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_session(f):\n",
    "    # use regex to look for the subject label and session label\n",
    "    \n",
    "    sub_search = re.search(r'sub-[a-zA-Z0-9]+', f)\n",
    "\n",
    "    assert sub_search.group(0)\n",
    "\n",
    "    subject = sub_search.group(0)\n",
    "\n",
    "\n",
    "    ses_search = re.search(r'ses-[a-zA-Z0-9]+', f)\n",
    "\n",
    "    assert ses_search.group(0)\n",
    "\n",
    "    session = ses_search.group(0)\n",
    "\n",
    "    # retun as a tuple\n",
    "    return(subject, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sub-2791617373', 'ses-PNC1')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_subject_session(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and a function to create a dictionary of acquisition labels extracted from the file names (since some acquisitions can have more than one file in them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acquisition_label(f):\n",
    "    \n",
    "    # get the subject and session labels so that we can insert them into a regex search string\n",
    "    subject, session = get_subject_session(f)\n",
    "\n",
    "    # use pathlib to get the stem (filename without slashes and extensions)\n",
    "    stem = pathlib.Path(f).stem    \n",
    "    \n",
    "    # sometimes we need to keep removing extensions, e.g. 'file.nii.gz'; use a while loop\n",
    "    while '.' in stem:\n",
    "        stem = pathlib.Path(stem).stem\n",
    "\n",
    "    # create the regex search string\n",
    "    regex = '(?<={}_{}_).+'.format(subject, session)\n",
    "\n",
    "    # run regex\n",
    "    acquisition_label = re.search(regex, stem).group(0)\n",
    "    return(acquisition_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m0scan',\n",
       " 'acq-se_asl',\n",
       " 'acq-gre_asl',\n",
       " 'acq-gre_aslcontext',\n",
       " 'acq-gre_asl',\n",
       " 'acq-se_asl',\n",
       " 'acq-se_aslcontext',\n",
       " 'm0scan']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquisitions = [create_acquisition_label(x) for x in files]    \n",
    "acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over the files, creating acquisitions and assigning the files to each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisitions = {}\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    acq = create_acquisition_label(f)\n",
    "    \n",
    "    if acq not in acquisitions.keys():\n",
    "        acquisitions[acq] = [f]\n",
    "    else:\n",
    "        acquisitions[acq].append(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m0scan': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.json',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.nii.gz'],\n",
       " 'acq-se_asl': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.json',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.nii.gz'],\n",
       " 'acq-gre_asl': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.nii.gz',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.json'],\n",
       " 'acq-gre_aslcontext': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_aslcontext.tsv'],\n",
       " 'acq-se_aslcontext': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_aslcontext.tsv']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a special case here: we need our `aslcontext.tsv` file to be uploaded to the session object -- we add a special case adjustment to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_acquisition_label(f):\n",
    "\n",
    "       \n",
    "    # get the subject and session labels so that we can insert them into a regex search string\n",
    "    subject, session = get_subject_session(f)\n",
    "\n",
    "    # use pathlib to get the stem (filename without slashes and extensions)\n",
    "    stem = pathlib.Path(f).stem    \n",
    "    \n",
    "    # sometimes we need to keep removing extensions, e.g. 'file.nii.gz'; use a while loop\n",
    "    while '.' in stem:\n",
    "        stem = pathlib.Path(stem).stem\n",
    "\n",
    "    # remove the word 'context' from the stem\n",
    "    if 'context' in stem:\n",
    "        stem = stem[:-7]\n",
    "    \n",
    "    # create the regex search string\n",
    "    regex = '(?<={}_{}_).+'.format(subject, session)\n",
    "\n",
    "    # run regex\n",
    "    acquisition_label = re.search(regex, stem).group(0)\n",
    "    return(acquisition_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisitions = {}\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    acq = create_acquisition_label(f)\n",
    "    \n",
    "    # if the key does not exist, create it and assign the value as a list with this file\n",
    "    if acq not in acquisitions.keys():\n",
    "        acquisitions[acq] = [f]\n",
    "    # otherwise, if the key exists, append the file to that list of files\n",
    "    else:\n",
    "        acquisitions[acq].append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m0scan': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.json',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.nii.gz'],\n",
       " 'acq-se_asl': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.json',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.nii.gz',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_aslcontext.tsv'],\n",
       " 'acq-gre_asl': ['/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.nii.gz',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_aslcontext.tsv',\n",
       "  '/storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.json']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acquisitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now, we need to work on targeting the session object and creating the acquisition object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use flywheel's finders to target the exact object we're looking for\n",
    "\n",
    "subject, session = get_subject_session(files[0])\n",
    "\n",
    "project = client.projects.find_first('label=RBC_PNC')\n",
    "subject = project.subjects.find_first('label={}'.format(subject))\n",
    "session = subject.sessions.find_first('label={}'.format(session))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "This session object has the method `add_acquisition`; you can just use this to label and upload data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing acquisition: m0scan\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.json\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.nii.gz\n",
      "\n",
      "Processing acquisition: acq-se_asl\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.json\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.nii.gz\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_aslcontext.tsv\n",
      "\n",
      "Processing acquisition: acq-gre_asl\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.nii.gz\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_aslcontext.tsv\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in acquisitions.items():\n",
    "    print('Processing acquisition:', k)\n",
    "\n",
    "    new_acquisition = session.add_acquisition(label=\"{}\".format(k))\n",
    "    \n",
    "    for file_upload in v:\n",
    "        \n",
    "        print('Uploading file ', file_upload)\n",
    "        new_acquisition.upload_file(\"{}\".format(file_upload))\n",
    "        new_acquisition = new_acquisition.reload() # update your copy of the object\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, though, we don't want to upload the `json` sidecar to the acquisition -- instead, we want to add this `json` as _metadata_ to the nifti file. To do this we'll read in the `json` data and convert it to a dictionary that Flywheel can understand and ingest, including BIDS fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata(file_list):\n",
    "    # build a dictionary of metadata to add to the file\n",
    "    \n",
    "    # get the json file and nifti file from this acquisition\n",
    "    json_file = [f for f in file_list if f.endswith('.json')][0]\n",
    "    nifti_file = [f for f in file_list if '.nii' in f][0]\n",
    "\n",
    "    # open it\n",
    "    with open(json_file, 'r') as read_file:\n",
    "            json_data = json.load(read_file)\n",
    "\n",
    "    # add important BIDS fields\n",
    "    bids = {\n",
    "        'Acq': '',\n",
    "        'Dir': '',\n",
    "        'Filename': '',\n",
    "        'Folder': '',\n",
    "        'ignore': False,\n",
    "        'Modality': '',\n",
    "        'Path': '',\n",
    "        'Run': ''\n",
    "    }\n",
    "\n",
    "    # to fill these BIDS fields, we extract them from the filename:\n",
    "    def find_value(string, key):\n",
    "\n",
    "        regex = r'(?<={}-)[a-zA-Z0-9]+'.format(key)\n",
    "\n",
    "        target_key = re.search(regex, string)\n",
    "\n",
    "        try:\n",
    "            return target_key.group(0)\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    bids['Acq'] = find_value(nifti_file, 'acq')\n",
    "    bids['Dir'] = find_value(nifti_file, 'dir')\n",
    "    bids['Run'] = find_value(nifti_file, 'run')\n",
    "\n",
    "    bids['Filename'] = pathlib.Path(nifti_file).name\n",
    "    bids['Folder'] = pathlib.Path(nifti_file).parents[0].name\n",
    "    bids['Path'] = '/'.join(list(pathlib.Path(nifti_file).resolve().parts[-4:-1]))\n",
    "    mod = create_acquisition_label(pathlib.Path(nifti_file).stem)\n",
    "    bids['Modality'] = mod[mod.find('_')+1:]\n",
    "    \n",
    "    json_data['BIDS'] = bids\n",
    "    return(json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AcquisitionDateTime': '2012-02-24T18:19:36.037500',\n",
       " 'AcquisitionDuration': 123,\n",
       " 'AcquisitionMatrixPE': 96,\n",
       " 'AcquisitionNumber': 1,\n",
       " 'AcquisitionTime': '18:19:36.037500',\n",
       " 'AverageB1LabelingPulses': 0,\n",
       " 'AverageLabelingGradient': 34,\n",
       " 'BackgroundSuppression': 'Yes',\n",
       " 'BandwidthPerPixelPhaseEncode': 30.193,\n",
       " 'BaseResolution': 96,\n",
       " 'BolusCutOffDelayTime': 0,\n",
       " 'BolusCutOffFlag': 'False',\n",
       " 'BolusCutOffTechnique': 'False',\n",
       " 'BolusCutOffTimingSequence': 'False',\n",
       " 'ConversionSoftware': 'dcm2niix',\n",
       " 'ConversionSoftwareVersion': 'v1.0.20180918  (JP2:OpenJPEG) GCC4.8.4',\n",
       " 'DeidentificationMethod': 'Penn_BSC_profile_v1.0',\n",
       " 'DerivedVendorReportedEchoSpacing': 0.000690005,\n",
       " 'DeviceSerialNumber': '35069',\n",
       " 'EchoTime': 0.029,\n",
       " 'EffectiveEchoSpacing': 0.000345003,\n",
       " 'FlipAngle': 90,\n",
       " 'ImageOrientationPatientDICOM': [1, 0, 0, 0, 1, 0],\n",
       " 'ImageType': ['ORIGINAL', 'PRIMARY', 'M', 'ND', 'MOSAIC'],\n",
       " 'InPlanePhaseEncodingDirectionDICOM': 'COL',\n",
       " 'InstitutionAddress': 'Spruce_Street_3400_Philadelphia_3438c3_Pennsylvania_US_19104',\n",
       " 'InstitutionName': 'HUP6',\n",
       " 'InterPulseSpacing': 4,\n",
       " 'LabelingDistance': 2,\n",
       " 'LabelingDuration': 1.5088,\n",
       " 'LabelingEfficiency': 0.72,\n",
       " 'LabelingOrientation': '',\n",
       " 'LabelingSlabLocation': 'X',\n",
       " 'LabelingSlabThickness': 2,\n",
       " 'LabelingType': 'PCASL',\n",
       " 'LookLocker': 'True',\n",
       " 'M0': 1,\n",
       " 'MRAcquisitionType': '2D',\n",
       " 'MagneticFieldStrength': 3,\n",
       " 'Manufacturer': 'Siemens',\n",
       " 'ManufacturersModelName': 'TrioTim',\n",
       " 'Modality': 'MR',\n",
       " 'PASLType': '',\n",
       " 'PCASLType': 'balanced',\n",
       " 'ParallelReductionFactorInPlane': 2,\n",
       " 'PartialFourier': 0.875,\n",
       " 'PatientPosition': 'HFS',\n",
       " 'PatientSex': 'F',\n",
       " 'PercentPhaseFOV': 100,\n",
       " 'PhaseEncodingDirection': 'j-',\n",
       " 'PhaseEncodingSteps': 83,\n",
       " 'PhaseResolution': 1,\n",
       " 'PixelBandwidth': 1929,\n",
       " 'PostLabelingDelay': 1.2,\n",
       " 'ProcedureStepDescription': 'mri_brain',\n",
       " 'ProtocolName': 'ep2d_se_pcasl_PHC_1200ms',\n",
       " 'PulseDuration': 1.5088,\n",
       " 'PulseSequenceDetails': 'WIP',\n",
       " 'PulseSequenceType': '2D',\n",
       " 'ReceiveCoilName': '32Ch_Head',\n",
       " 'ReconMatrixPE': 96,\n",
       " 'RepetitionTime': 4,\n",
       " 'SAR': 0.620302,\n",
       " 'ScanOptions': 'PFP_FS',\n",
       " 'ScanningSequence': 'EP',\n",
       " 'SequenceName': 'epse2d1_96',\n",
       " 'SequenceVariant': 'SK_SP',\n",
       " 'SeriesDescription': 'ep2d_se_pcasl_PHC_1200ms',\n",
       " 'SeriesInstanceUID': '1.3.12.2.1107.5.2.32.35069.2012022418191982204684827.0.0.0',\n",
       " 'SeriesNumber': 5,\n",
       " 'ShimSetting': [4029, -6670, -3873, 1272, -51, 4, -188, 72],\n",
       " 'SliceSelectiveLabelingGradient': 45,\n",
       " 'SliceThickness': 5,\n",
       " 'SliceTiming': [0,\n",
       "  0.06,\n",
       "  0.1225,\n",
       "  0.1825,\n",
       "  0.245,\n",
       "  0.305,\n",
       "  0.365,\n",
       "  0.4275,\n",
       "  0.4875,\n",
       "  0.55,\n",
       "  0.61,\n",
       "  0.67,\n",
       "  0.7325,\n",
       "  0.7925,\n",
       "  0.8525,\n",
       "  0.915,\n",
       "  0.975,\n",
       "  1.0375,\n",
       "  1.0975,\n",
       "  1.1575],\n",
       " 'SoftwareVersions': 'syngo_MR_B17',\n",
       " 'SpacingBetweenSlices': 6,\n",
       " 'StationName': 'hup6',\n",
       " 'StudyID': '8317731',\n",
       " 'StudyInstanceUID': '1.2.840.113745.101000.1002000.40959.4056.32654864',\n",
       " 'TotalReadoutTime': 0.0327753,\n",
       " 'TxRefAmp': 312.457,\n",
       " 'BIDS': {'Acq': 'se',\n",
       "  'Dir': '',\n",
       "  'Filename': 'sub-2791617373_ses-PNC1_acq-se_asl.nii.gz',\n",
       "  'Folder': 'perf',\n",
       "  'ignore': False,\n",
       "  'Modality': '',\n",
       "  'Path': 'sub-2791617373/ses-PNC1/perf',\n",
       "  'Run': ''}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_metadata(acquisitions['acq-se_asl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Now we can add this to our loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing acquisition: m0scan\n",
      "Building acquisition metadata\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_m0scan.nii.gz\n",
      "Updating nifti metadata...\n",
      "\n",
      "Processing acquisition: acq-se_asl\n",
      "Building acquisition metadata\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-se_asl.nii.gz\n",
      "Updating nifti metadata...\n",
      "Updating TSV metadata...\n",
      "Uploading TSV to session\n",
      "\n",
      "Processing acquisition: acq-gre_asl\n",
      "Building acquisition metadata\n",
      "Uploading file  /storage/ttapera/RBC/data/131160/sub-2791617373/ses-PNC1/perf/sub-2791617373_ses-PNC1_acq-gre_asl.nii.gz\n",
      "Updating nifti metadata...\n",
      "Updating TSV metadata...\n",
      "Uploading TSV to session\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in acquisitions.items():\n",
    "    print('Processing acquisition:', k)\n",
    "\n",
    "    new_acquisition = session.add_acquisition(label=\"{}\".format(k))\n",
    "    \n",
    "    print('Building acquisition metadata')\n",
    "    \n",
    "    meta = build_metadata(v)\n",
    "\n",
    "    for file_upload in v:\n",
    "        \n",
    "        # we no longer need to upload jsons\n",
    "        if '.json' in file_upload:\n",
    "            continue\n",
    "            \n",
    "        # upload TSVs straight to the session object\n",
    "        elif 'context' in file_upload:\n",
    "            print('Updating TSV metadata...')\n",
    "            # the context file doesn't need as much data, \n",
    "            # just the BIDS Filename, Path, and Folder\n",
    "            subset_meta = meta['BIDS']\n",
    "            subset_meta['Filename'] = subset_meta['Filename'].replace('asl.nii.gz', 'aslcontext.tsv')\n",
    "            \n",
    "            print('Uploading TSV to session')\n",
    "            session.upload_file(file_upload)\n",
    "            session = session.reload()\n",
    "            session.update_file_info(subset_meta['Filename'], {'BIDS': subset_meta})\n",
    "            session = session.reload()\n",
    "        \n",
    "        # upload the nifti to the new acquisition\n",
    "        elif '.nii' in file_upload:\n",
    "            print('Uploading file ', file_upload)\n",
    "            new_acquisition.upload_file(\"{}\".format(file_upload))\n",
    "            new_acquisition = new_acquisition.reload() # update your copy of the object\n",
    "            \n",
    "            # update the metadata\n",
    "            print('Updating nifti metadata...')\n",
    "            new_acquisition.update_file_info(meta['BIDS']['Filename'], meta)\n",
    "            new_acquisition = new_acquisition.reload() # update your copy of the object\n",
    "\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap all of the above in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_upload_asl(args):\n",
    "    \n",
    "    # step 1: get files\n",
    "    \n",
    "    path = '/storage/ttapera/RBC/data/{}/*/*/*/*'.format(args[1])\n",
    "    files = glob.glob(path)\n",
    "    \n",
    "    # step 2: get the asl acquisitions\n",
    "    acquisitions = {}\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        acq = create_acquisition_label(f)\n",
    "\n",
    "        # if the key does not exist, create it and assign the value as a list with this file\n",
    "        if acq not in acquisitions.keys():\n",
    "            acquisitions[acq] = [f]\n",
    "        # otherwise, if the key exists, append the file to that list of files\n",
    "        else:\n",
    "            acquisitions[acq].append(f)\n",
    "            \n",
    "    # step 3: extract subject and session labels; initialise flywheel target object\n",
    "    \n",
    "    subject, session = get_subject_session(files[0])\n",
    "\n",
    "    project = client.projects.find_first('label=RBC_PNC')\n",
    "    subject = project.subjects.find_first('label={}'.format(subject))\n",
    "    session = subject.sessions.find_first('label={}'.format(session))\n",
    "    \n",
    "    # step 4: upload data\n",
    "    \n",
    "    for k,v in acquisitions.items():\n",
    "        print('Processing acquisition:', k)\n",
    "\n",
    "        new_acquisition = session.add_acquisition(label=\"{}\".format(k))\n",
    "\n",
    "        print('Building acquisition metadata')\n",
    "\n",
    "        meta = build_metadata(v)\n",
    "\n",
    "        for file_upload in v:\n",
    "\n",
    "            # we no longer need to upload jsons\n",
    "            if '.json' in file_upload:\n",
    "                continue\n",
    "\n",
    "            # upload TSVs straight to the session object\n",
    "            elif 'context' in file_upload:\n",
    "                print('Updating TSV metadata...')\n",
    "                # the context file doesn't need as much data, \n",
    "                # just the BIDS Filename, Path, and Folder\n",
    "                subset_meta = meta['BIDS']\n",
    "                subset_meta['Filename'] = subset_meta['Filename'].replace('asl.nii.gz', 'aslcontext.tsv')\n",
    "\n",
    "                print('Uploading TSV to session')\n",
    "                session.upload_file(file_upload)\n",
    "                session = session.reload()\n",
    "                session.update_file_info(subset_meta['Filename'], {'BIDS': subset_meta})\n",
    "                session = session.reload()\n",
    "\n",
    "            # upload the nifti to the new acquisition\n",
    "            elif '.nii' in file_upload:\n",
    "                print('Uploading file ', file_upload)\n",
    "                new_acquisition.upload_file(\"{}\".format(file_upload))\n",
    "                new_acquisition = new_acquisition.reload() # update your copy of the object\n",
    "\n",
    "                # update the metadata\n",
    "                print('Updating nifti metadata...')\n",
    "                new_acquisition.update_file_info(meta['BIDS']['Filename'], meta)\n",
    "                new_acquisition = new_acquisition.reload() # update your copy of the object\n",
    "\n",
    "\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
